# VisionAndLanguage-Hub
Coustomized curated list of Vision and Language works

## Videos-Hub

| Name      | Paper |   Code  |
| ----------- | ----------- | ----------- |
| VindLU: A Recipe for Effective Video-and-Language Pretraining  | [Text](arxiv.org/abs/2212.05051)  | [code](github.com/klauscc/VindLU)  |
| MAGVIT: Masked Generative Video Transformer   | [Text](https://arxiv.org/abs/2212.05199 ) | [project page](magvit.cs.cmu.edu) |
|Video-Text Modeling with Zero-Shot Transfer from Contrastive Captioners  |[paper](https://arxiv.org/abs/2212.04979)| -  |
|PromptonomyViT: Multi-Task Prompt Learning Improves Video Transformers using Synthetic Scene Data | [paper](arxiv.org/abs/2212.04821)| - |
|Learning Video Representations from Large Language Models|[paper](https://arxiv.org/abs/2212.04501) [page](https://t.co/dh2DD1NDaU)|[code](https://github.com/facebookresearch/LaViLa)|


## Vision&Language-Hub

| Name      | Paper |   Code  |
| ----------- | ----------- | ----------- |
|REVEAL: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory |[paper](arxiv.org/abs/2212.05221)|-|

## Constrative Image-Language Pretraining (CLIP)

|Name | Paper | Code |
| ----------- | ----------- | ----------- |
|CLIP Itself is a Strong Fine-tuner: Achieving 85.7% and 88.0% Top-1 Accuracy with ViT-B and ViT-L on ImageNet| [Paper](arxiv.org/abs/2212.06138)| - |


## Miscellaneous but Interesting!
|Name | Paper | Code |
| ----------- | ----------- | ----------- |
|ULIP: Learning Unified Representation of Language, Image and Point Cloud for 3D Understanding |[paper](arxiv.org/abs/2212.05171)|[page](tycho-xue.github.io/ULIP/)|

